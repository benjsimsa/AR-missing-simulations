---
title: "Discussion"
output: pdf_document
bibliography: references.bib
---

\newpage

# Discussion

We conducted two Monte Carlo simulation studies to address a gap in knowledge about the influence of missing data on the estimation performance of the multilevel autoregressive model. In Simulation A, we only estimated and simulated fixed autoregressive effects (together with both fixed and random intercepts), while in Simulation B, both fixed and random autoregressive effects were simulated and estimated. Three outcomes were evaluated in both simulations: the estimation bias, standard error of the simulations, and statistical power. Four values of compliance and four missingness patterns (data MCAR, data missing in a block of consecutive observations, the highest values missing, and the highest and lowest values missing) were varied across the simulations. The other manipulated factors included the number of participants, the number of time points per participant, the simulated value of the fixed AR effect, and the variance of the random AR effects.

The two parameters related to missing data (compliance and missingness pattern) emerged as very important factors influencing all three outcomes. In both simulations, missingness type and compliance (and the interaction between the two) were the factors with the largest effect on the bias in the estimation of the fixed AR effect. Similarly, both missingness type and compliance had a strong influence on the statistical power to detect the fixed AR effect in both simulations. With regards to the standard error of the simulation results, compliance was found to have a very large effect (more so in Simulation A than in Simulation B), while the effect of missingness type was only moderate.

Our results corroborate the conclusions about the importance of the number of time points per participant for precise estimation of the autoregressive effects [@hamaker2015; @krone2016]. In general, the estimation bias became considerably less severe as the ESM time-series length per participant increased. *T.obs* also had a large effect on statistical power. However, our simulations show that the context of missingness matters: when the compliance is low and the data are missing MCAR or in blocks, the underestimation of the fixed AR effect caused by the missing data (and the negative consequences for statistical power) becomes less severe very quickly as *T.obs* increases. On the other hand, when the missingness is dependent on the value of the process itself (i. e., the most extreme observations are missing), increasing compliance appears to be more important for estimation precision and statistical power than the length of the time-series. In other words, it can be said that the presence of missing data exacerbates Nickell's bias [@nickell1981] - an estimation bias introduced by person-mean centering in multilevel models.

While Krone et al. [-@krone2016] found that estimation bias becomes smaller as the simulated fixed AR effect becomes larger, we found an opposite pattern: estimation bias was larger as the simulated fixed AR effect became larger.

Overall, there was always some degree of estimation bias present in the simulations, ranging from very severe (when *T.obs* and compliance were low, and the missingness of data was dependent on the process value) to mild (when compliance was high and the data were missing either MCAR or in block). This bias in estimation might be one of the driving forces behind the low value added by estimates of emotional inertia to the prediction of psychopathology and well--being, pointed out by Dejonckheere et al. [-@dejonckheere2019]. Additionally, while the simulation studies did not explicitly assess the bias in the estimation of individual autoregressive effects, the results suggest that some individual differences in inertia estimates might not be caused by real differences in inertia, but due to the bias caused by missingness: for two individual participants with an identical real autoregressive parameter but different compliance and missingness patterns, the inertia estimates can vary considerably.

The supplementary simulation shows that the estimation bias is considerably less severe (and the standard errors are smaller) when the predictor is not person-mean centered. These results suggest that while not person-mean centering the predictor makes the interpretation of the autoregressive parameters and the intercepts more challenging, it might be the optimal choice when the number of observations and/or the compliance is low. However, as Hamaker & Grasman [-@hamaker2015] point out, the choice between centering and not centering the predictor should primarily be guided by the researchers' goals. If the researchers aim to obtain itnerpretable intercepts or investigating how a Level 2 predictor influences the autoregression, person-mean centering might still be preferable. On the other hand, the results of the present thesis support Hamaker and Grasman's (2015) claim that if the focal point of interest is the fixed autoregressive parameter, it is better to avoid person-mean centering the predictor.

Our results have several implications for the design choices in psychological research using the multilevel autoregressive model to estimate emotional inertia. First, in line with previous simulation studies, we recommend for researchers to focus on increasing *T.obs* rather than *N* in order to increase the statistical power and the precision of the inertia estimates. In other words, for optimal statistical performance, it is more effective to make the data collection period longer (or schedule more beeps per day) than to collect data from more participants. Secondly, while the time-series length is important, researchers should aim to design their ESM studies in a way that will make compliance as high as possible. According to recent evidence about compliance in ESM studies, these design choices include providing financial incentives to participants [@wrzus2022] and including less items in ESM questionnaires [@eisele2020]. Furthermore, the results suggest that the potential presence of missing data should be accounted for in power analyses for ESM studies. In an ideal case, a researcher should have an idea about what the average compliance in their study could be and what missingness patterns might be present in the data. Of course, this is not entirely feasible, as it might be difficult to estimate the average compliance, and real-life ESM data will likely include a mixture of different missing data patterns, both at the within- and between-person level. Still, to avoid overestimating statistical power for planned studies, it is advisable to include several different missing data scenarios in the power simulations as a sensitivity check.

### Directions for future research

The present thesis provides evidence about estimation bias being made more severe by lower compliance and by patterns of missingness that depend on the process value itself. Nonetheless, the insight into the mechanisms driving this bias remains limited. Several plausible explanations of the bias arise. First, the estimation bias was the most severe when the most extreme observations at both ends of the ESM process distribution were missing. This finding can be linked to the evidence about tails (i. e., the extreme ends) of distributions containing the most information about the scale of the distribution [@zheng2002]. However, further investigation is needed to provide more understanding about whether the tails also provide crucial information about the autoregressive effect.

Secondly, the different bias values for different missingness patterns could be partly caused by the fact that the number of "effective" observation-pairs (i. e., pairs of current and lagged values where both values are not missing) used to estimate the autoregressive parameters differs between the missignness patterns. For example, the number of effective observation-pairs will be larger when the data are MCAR than when they are missing in a block. However, the results of the present thesis seem to contradict this explanation. On the one hand, estimation bias was found to be very similar when data are MCAR and missing in blocks (although the two missingness patterns come with a very different number of observation-pairs). On the other hand, the estimation bias was much larger for the conditions with the most extreme values set as missing, compared to the condition with data MCAR (even though the number of effective observation-pairs is generally higher for the "extreme" conditions than when data are MCAR). As such, this explanation of estimation bias appears to be less plausible than the explanation described in the previous paragraph.

### Limitations

First, while the two simulation studies include a wide range of scenarios and parameter combinations, our results are far from comprehensive, and they largely depend on the simulation parameters. However, the reproducible code available from the GitHub repository (<https://github.com/benjsimsa/AR-missing-simulations>) provides a sufficient framework for an interested reader to rerun the analyses with different parameters and modify the code to better fit the peculiarities of their specific study sample and research questions.

Furthermore, the results depend on several assumptions, which were problematised as being too simplistic in previous research. We assumed that the innovation variance $\sigma$ was identical for all the participants, and that the random intercepts (and random slopes in Simulation B) came from a normal distribution. Additionally, we only focused on normally distributed affective processes. While a normal distribution can be assumed for ESM measures of positive emotions, negative affective processes are usually heavily right-skewed in the general population [@haslbeck2022]. Additionally, we assumed that the analysed ESM time-series are measured without any error; however, recent evidence shows that this is very often not the case in real-world research [@dejonckheere2022; @schuurman2019], and unreliability can lead to further attenuation of the estimated parameters [@wenzel2022]. While person-mean centering was carried out using observed means in the simulations we conducted, different ways of person-mean centering, such as using latent person-means, might be more appropriate [@gistelinck2021]. Another assumption of the present simulations that is unlikely to hold in real-world data is the homogeneity of compliances and missing data patterns within each simulated dataset. In the real world, it can be expected for different missingness patterns to be present in the data at both the between- and within-person level.

Finally, although we took steps to make the simulations reproducible by making all code and results publicly available, using R packages *here* and *renv*, and reporting the sessionInfo for every simulation, a large number of packages with many dependencies were used, which might be detrimental to reproducibility in the long term.
