---
title: "Discussion"
output: pdf_document
bibliography: references.bib
---

\newpage

# Discussion

We conducted two Monte Carlo simulation studies to address a gap in knowledge about the influence of missing data on the estimation performance of the multilevel autoregressive model. In Simulation A, we only estimated and simulated fixed autoregressive effects (together with both fixed and random intercepts), while both fixed and random autoregressive effects were simulated and estimated in Simulation B. Three outcomes were evaluated in both simulations: the estimation bias, standard error of the simulations, and statistical power. Four values of compliance and four different missingness patterns (data MCAR, data missing in a block of consecutive observations, all values below a given percentile missing, and the most extreme / highest and lowest values missing) were varied across the simulations. The other manipulated factors included the number of participants, the number of time points per participant, the simulated value of the fixed AR effect, and the variance of the random AR effects.

The two simulation parameters related to missing data (compliance and missingness pattern) emerged as very important factors influencing all three outcomes. In both simulations, missingness type and compliance (and the interaction between the two) were the factors with the largest effect on the bias in the estimation of the fixed AR effect. Similarly, both missingness type and compliance had a strong influence on the statistical power to detect the fixed AR effect in both simulations. With regards to the standard error of the simulation results, compliance was found to have a very large effect (more so in Simulation A than in Simulation B), while the effect of missingness type was only moderate.

Our results corroborate the conclusions about the importance of the number of time points per participant for precise estimation of the autoregressive effects [@hamaker2015; @krone2016]. In general, the estimation bias became considerably less severe as the ESM time-series length per participant increased. The number of observations per participant also had a large effect on statistical power. However, our simulations show that the context of missingness matters: when the compliance is low and the data are missing MCAR or in blocks, the underestimation of the fixed AR effect caused by the missing data (and the negative consequences for statistical power) becomes less severe very quickly as the number of observations per participant increases. On the other hand, when the missingness is dependent on the value of the process itself (i. e., the most extreme observations are missing), increasing compliance appears to be more important for estimation precision and statistical power than the length of the time-series. In other words, the presence of missing data exacerbates Nickell's bias [@nickell1981] - an estimation bias introduced by person-mean centering in multilevel models.

While Krone et al. [-@krone2016] found that estimation bias becomes smaller as the simulated fixed AR effect increases, we found an opposite pattern: overall, estimation bias increased with increasing simulated fixed AR effect. However, this only held true for the two missingness patterns in which the most extreme observations were set as missing. In conditions with data missing MCAR or in block, the estimation bias stayed almost identical regardless of the magnitude of the simulated fixed AR effect (in Simulation A) or increased slightly (Simulation B). The discrepancy between our results and those by Krone et al. (2016) can be explained by the fact that Krone et al. did not focus on missing data in their simulations.

Overall, there was always some degree of estimation bias present in the simulations, ranging from very severe (when the number of timepoints per participant and compliance were low, and the missingness of data was dependent on the process value) to mild (when compliance was high and the data were missing either MCAR or in block). The downward bias was especially pronounced in the simulations that used person-mean centering for the lagged predictor. The estimation bias resulting from the presence of missing data might be one of the driving forces behind the low value added by estimates of emotional inertia to the prediction of psychopathology and well--being, pointed out by Dejonckheere et al. [-@dejonckheere2019]. Additionally, while the simulation studies did not explicitly assess the bias in the estimation of individual autoregressive effects, the results suggest that some individual differences in inertia estimates might not be caused by real differences in inertia, but due to the bias caused by missingness: for two individual participants with an identical real autoregressive parameter but different compliance and missingness patterns, the inertia estimates can vary considerably.

The supplementary simulation shows that the estimation bias is considerably less severe (and the standard errors are smaller) when the lagged predictor is not person-mean centered. These results suggest that while not person-mean centering the predictor makes the interpretation of the autoregressive parameters and the intercepts more challenging, it might be the optimal choice when the number of observations and/or the compliance is low. However, as Hamaker & Grasman [-@hamaker2015] point out, the choice between centering and not centering the predictor should primarily be guided by the researchers' goals. If the researchers aim to obtain interpretable intercepts or to investigate how a Level 2 predictor influences the autoregressive effect, person-mean centering might still be preferable. On the other hand, the results of the present thesis support Hamaker and Grasman's (2015) claim that if the focal point of interest is the fixed autoregressive parameter, it is better to avoid person-mean centering the predictor.

Our results have several implications for the design choices in psychological studies that use the multilevel autoregressive model to estimate emotional inertia. First, in line with previous simulation studies [@krone2016], we recommend for researchers to focus on increasing the number of timepoints per participant, rather than the number of participants, in order to increase the statistical power and the precision of the inertia estimates in situations where some amount of missing data can be expected. In other words, for optimal statistical performance, it is more effective to make the data collection period longer (or schedule more beeps per day) than to collect data from more participants. Secondly, while the time-series length is important, researchers should aim to design their ESM studies in a way that will make compliance as high as possible. According to recent evidence about compliance in ESM studies, these design choices include providing financial incentives to participants [@wrzus2022] and including less items in the ESM questionnaires [@eisele2020]. Furthermore, the results suggest that the potential presence of missing data should be accounted for in power analyses for ESM studies. In an ideal case, a researcher should have an idea about the potential average compliance in their study and the missingness patterns that can be expected in the data. Of course, this is not entirely feasible, as it might be difficult to estimate the average compliance, and real-life ESM data will likely include a mixture of different missing data patterns, both at the within- and between-person level. Still, to avoid overestimating statistical power for planned studies, it is advisable to include several different missing data scenarios in the power simulations as a sensitivity check.

### Directions for future research

The present thesis provides evidence about estimation bias being made more severe by lower compliance and by patterns of missingness that depend on the process value itself. Nonetheless, the insight into the mechanisms driving this bias remains limited. Several plausible explanations of the bias arise. First, the estimation bias was the most severe when the most extreme observations at both ends of the ESM process distribution were missing. This finding can be linked to the evidence about tails (i. e., the extreme ends) of distributions containing the most information about the scale of the distribution [@zheng2002]. However, further investigation is needed to provide a more detailed understanding about whether the tails also provide crucial information about the autoregressive effect.

Secondly, the different bias values for different missingness patterns could be partly caused by the fact that the number of "effective" observation-pairs (i. e., pairs of current and lagged values where both values are not missing) used to estimate the autoregressive parameters differs between the missignness patterns. For example, the number of effective observation-pairs will be smaller when the data are MCAR compared to when they are missing in a block. However, the results of the present thesis seem to contradict this explanation. On the one hand, estimation bias was found to be very similar when data are MCAR and missing in blocks (although the two missingness patterns come with a very different number of observation-pairs). On the other hand, the estimation bias was much larger for the conditions with the most extreme values set as missing, compared to the condition with data MCAR (even though the number of effective observation-pairs is generally higher for the "extreme" conditions than when data are MCAR). As such, this explanation of estimation bias appears to be less plausible than the explanation described in the previous paragraph.

### Limitations

First, while our simulation studies include a wide range of scenarios and parameter combinations, our results are far from comprehensive, and they largely depend on the simulation parameters. However, the reproducible code available from the GitHub repository (<https://github.com/benjsimsa/AR-missing-simulations>) provides a sufficient framework for an interested reader to rerun the analyses with different parameters and modify the code to better fit the peculiarities of their specific study sample and research questions.

Furthermore, the results depend on several assumptions, which were problematised as being too simplistic in previous research. We assumed that the innovation variance $\sigma$ was identical for all the participants, and that the random intercepts (and random slopes in Simulation B) came from a normal distribution. Additionally, we only focused on normally distributed affective processes. While a normal distribution can be assumed for ESM measures of positive emotions, negative affective processes are usually heavily right-skewed in the general population [@haslbeck2022]. Additionally, we assumed that the analysed ESM time-series are measured without any error; however, recent evidence shows that this is very often not the case in real-world research [@dejonckheere2022; @schuurman2019], and unreliability can lead to further attenuation of the estimated parameters [@wenzel2022]. While person-mean centering was carried out using observed means in the simulations we conducted, different ways of person-mean centering, such as using latent person-means, might be more appropriate [@gistelinck2021]. Another assumption of the present simulations that is unlikely to hold in real-world data is the homogeneity of compliances and missing data patterns within each simulated dataset. In the real world, it can be expected for different missingness patterns to be present in the data at both the between- and within-person level.

Finally, although we took steps to make the simulations reproducible by making all code and results publicly available, using R packages *here* and *renv*, and reporting the sessionInfo for every simulation, a large number of packages with many dependencies were used, which might be detrimental to reproducibility in the long term.

### Conclusion 

To sum up, our results suggest that the presence of missing observations in ESM datasets can introduce bias in the results of multilevel autoregressive models and jeopardize the conclusions about inertia (and individual differences therein) drawn from them. The magnitude of estimation bias is higher when the compliance is low, when the missingness is dependent on process value, and when the number of observations per participant is low. Furthermore, the estimation bias is more pronounced when the researchers person-mean the predictor in the MLAR model. To minimize the estimation bias, we recommend the researchers to design their ESM data collection procedures in ways that promote high compliance, and, if possible, use designs with large numbers of observations per participant.
