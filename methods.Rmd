---
title: "Methods"
output: pdf_document
bibliography: references.bib
header-includes:
  - \usepackage{amsmath}
---

# Multilevel AR(1) model

In this subchapter, I will describe the mathematical basis and assumptions of the first-order multilevel autoregressive model with random intercepts and random autoregressive effects, which will be the focus of the simulation part of the thesis. The notation used by Lafit et al. [-@lafit2020] will be adhered to throughout the thesis.

The MLAR model consists of two levels: the within-person Level 1 and the between-person Level 2. At Level 1, described by Equation \eqref{eq:level1} [@lafit2020], each participant's first-order autoregressive process is modelled: The person-specific autoregressive parameter $\gamma_{1i}$ quantifies to what degree the process value $esm_{it}$ of participant *i* at time *t* depends on the lagged process value $esm_{i, t-1}$. The person-specific intercept $\gamma_{0i}$ represents the expected process value $esm_{it}$ when the lagged variable $esm_{i, t-1}$ equals 0 [@jongerling2015]. The innovation $\epsilon_{it}$ (i.e., residuals, the part of the process variance that is not explained by the lagged variable $esm_{i, t-1}$) is assumed to be independent and coming from a normal distribution with mean of 0 and variance $\sigma^2_{\epsilon}$ (Lafit et al., 2020). The model used in the present thesis assumes the innovation variance to be identical for all participants.

```{=tex}
\begin{equation}\label{eq:level1}
    esm_{it}= \gamma_{0i} + \gamma_{1i} * esm_{i, t-1} + \epsilon_{it}
\end{equation}
```
In the multilevel AR(1) model, the person-specific autoregressive effects $\gamma_{1i}$ and the person-specific intercepts $\gamma_{0i}$ are allowed to vary between participants. The Level 2 of the MLAR(1) model describes this between-person variability. The Level 2 is defined in Equation \eqref{eq:level2}. Each person-specific autoregressive effect $\gamma_{1i}$ is a sum of a fixed effect $\beta_{10}$ and a person-specific random effect $\nu_{1i}$. The random effects come from a bivariate normal distribution with variances denoted as $\sigma_{\nu_0}^2$ and $\sigma_{\nu_1}^2$ and correlation $\sigma_{\nu_{01}}$. The person-specific intercepts $\gamma_{0i}$ are a sum of a fixed effect $\beta_{00}$ and a random effect $\nu_{0i}$ that comes from *N*(0, $\sigma^2_{\nu0}$).

```{=tex}
\begin{equation}\label{eq:level2}
\begin{aligned}
    \gamma_{0i} = \beta_{00} +  \nu_{0i} \\
    \gamma_{1i} = \beta_{10} +  \nu_{1i}
\end{aligned}
\end{equation}
```
To illustrate the process of estimating a multilevel autoregressive model, let us imagine a researcher who, similarly to Houben and Kuppens [-@houben2020], is interested in whether the inertia of negative affect is associated with borderline personality disorder features. To do so, she collects ESM data from 100 participants, obtaining self-reported values 10 times a day for 7 days via a mobile app.

The inertia of negative affect for each participant is operationalized as the person-specific autoregressive effect $\gamma_{1i}$, i.e., to what degree the intensity of negative affect at time *t* is influenced by the intensity at the previous timepoint, *t-1*. However, given the fluctuating nature of human affect, the magnitude of negative affect can almost never be perfectly predicted from its previous value. For example, the participant can be made feel worse by an external event, or actively use emotion regulation techniques to manage their negative feelings. This variability in negative affect that cannot be predicted from the previous affect magnitude is included in the innovation parameter $\epsilon_{it}$ [@jongerling2015].

Each participant's person-specific autoregressive effect $\gamma_{1i}$ consists of two components: the fixed effect $\beta_{10}$, which is identical across all participants, and the random effect $\nu_{1i}$, which is estimated for each participant individually. As such, the person-specific effects are assumed to differ while also being assumed to come from the same distribution. The estimates can then be used in a correlation analysis.

### Assumptions of the MLAR(1) model

In this part, the assumptions of the MLAR(1) model will be explained.

**Stationarity.** The MLAR(1) model is used to model stable processes in which no temporal trends (i.e., changes in the process mean over time) are present. As such, it assumes weak stationarity: the (person-specific) process mean, innovation variance, and autoregressive parameter are assumed to not change throughout the time series [@rovine2006]. Thus, the person-specific autoregressive effects $\gamma_{1i}$ are assumed to be bounded by -1 and 1, as autoregressive effects larger than 1 (or lower than -1) cause a change in the process mean [@krone2016].

**Equally spaced measurements.** The time-periods that elapsed between each pair of consecutive measurement occasions are assumed to be equal in the following simulation study. In real-life ESM data, the lagged value of the last observation of each day is usually set as missing to account for the fact that the gap between the last night beep and the first morning beep is much larger than the time-gap between the other observations.

### Estimation procedures

In the following subchapter, I will present two most-used approaches to estimating the MLAR (1) model in psychology: the maximum likelihood estimation (MLE) and Bayesian Markov Chain Monte Carlo (MCMC) estimation.

**Maximum likelihood estimation (MLE).** Thanks to its availability in standard software, such as the *nlme* R package [@pinheiro2022], ML is the most popular approach to estimating MLAR(1) models [@jongerling2015]. The Full Information Maximum Likelihood method, which includes the regression coefficients and the variance components in the likelihood, is usually used for the estimation in combination with the Broyden-Fletcher-Goldfarb-Shanno algorithm, specifically designed to estimate stationary autocorrelation parameters [@krone2016].

**Bayesian estimation.** Bayesian MCMC is a flexible way of estimating the MLAR(1) model [@krone2016]. When either the outcome or the lagged predictor is missing, it allows avoiding list-wise deletion of observation-pairs when either the outcome or the predictor are missing by using the estimated autoregressive parameter to estimate the value of the missing observations [@krone2016].

### Evidence from simulation studies

Multiple simulation studies about the statistical properties of the MLAR(1) model have been published in recent years. In this subchapter, I will summarise the most important findings.

Jongerling et al. [-@jongerling2015] found that modelling innovation variance as fixed (i.e., identical for all participants) instead of random when it actually differs across participants leads to a considerable bias in the estimation of the fixed AR effect. There is an upward bias (overestimation) present when the correlation between the individual AR effects and individual innovation variances is positive, and vice versa. Additionally, Jongerling et al. point out that using the person-means to center the lagged predictor variable leads to a downward bias in the estimation of the fixed AR effect. The effect of person-mean centering the predictor on the estimation performance of the MLAR model was further studied by Hamaker & Grasman [-@hamaker2015]. Their simulation study confirmed that person-mean centering leads to an underestimation of the fixed autoregressive effect, especially when the number of time points per participant *(T.obs)* is low. Still, they recommend using person-mean centering when one is interested in the effect of a between-person predictor on inertia.

In their simulation study comparing the maximum likelihood and Bayesian approaches to estimating the MLAR model, Krone et al. [-@krone2016] show that the two estimation procedures have a comparable performance. Furthermore, a higher number of time points per participant leads to more precise estimates, while the effect of the number of participants on the estimation performance is small. They also show that a higher variance of the random AR effects leads to a worse estimation precision and that the estimation bias gets smaller when the real fixed AR effect increases.

Liu [-@liu2017] assessed how violating the normality of the random AR effect distribution influences the estimation performance of the MLAR model. The different distributions of the random AR effects were found to only have a small effect on the estimation performance.

While the simulation studies mentioned above provide an extensive body of evidence about the statistical properties of the MLAR model under different conditions, several questions remain unanswered. One of them is the effect of missing observations on estimation performance. The presence of missing values in an intensive longitudinal dataset decreases the number of observations per participant (or, more specifically, the number of observation-pairs that can be used for the estimation of the model). As such, it can be expected that lower compliance (i.e., lower proportion of ESM beeps that the given participant answered) will make estimation bias more severe. Additionally, different patterns of missingness might have different consequences on the estimation performance. Ji et al. [-@ji2018] show that the presence of data missing completely at random (MCAR), missing at random (MAR) and not missing at random (MNAR) leads to a considerable bias in point estimates of cross-lagged and autoregressive parameters in vector autoregressive models when list-wise deletion is used. However, no similar evidence is available about the MLAR(1) model.

\newpage

# Methods

The goal of the present exploratory simulation study is to assess the effects of four different patterns of missing data (data missing completely random, data missing in blocks, and two patterns of missingness dependent on process value; see Figure \ref{fig:fig_miss_comparison}) and study compliance on estimation performance and bias, standard error, and statistical power for the estimation of the fixed autoregressive effect in the MLAR(1) model. Apart from the missingness patterns and compliance, we manipulated the number of participants, the number of time points per participant, the simulated fixed autoregressive effect, and the variance of random AR effects. The values of the manipulated variables for both studies are reported in Table \ref{tab:tab_manipulated}. The values of the manipulated variables were set considering realistic research questions in psychology. The study was exploratory; no apriori hypotheses were tested.

### Simulation procedure

The study followed the general principles of the Monte Carlo simulation procedure described by Lane & Hennes [-@lane2018].

**Simulation conditions.** Two simulation studies, Simulation A and Simulation B, were carried out to investigate the research questions. In Simulation A, no random autoregressive effects were simulated and estimated (i.e., each subject's time-series had the same simulated autoregressive effect, and only fixed autoregressive effects were estimated). In Simulation B, random autoregressive effects were simulated and estimated, with the random effects variance set to either 0.05 or 0.1 and the correlation between the random slopes and random intercepts set to 0. Both random and fixed intercepts were estimated in Simulations A and B. The multilevel autoregressive model estimated in Simulation A is defined in Equation \eqref{eq:sim_a}, while Equation \eqref{eq:sim_b} describes the model estimated in Simulation B.

```{=tex}
\begin{equation}\label{eq:sim_a}
\begin{aligned}
    esm_{it} = \gamma_{0i} + \gamma_{1i} * esm_{i, t-1} + \epsilon_{it} \\
    \gamma_{0i} = \beta_{00} +  \nu_{0i} \\
    \gamma_{1i} = \beta_{10}
\end{aligned}
\end{equation}
```
```{=tex}
\begin{equation}\label{eq:sim_b}
\begin{aligned}
    esm_{it} = \gamma_{0i} + \gamma_{1i} * esm_{i, t-1} + \epsilon_{it} \\
    \gamma_{0i} = \beta_{00} +  \nu_{0i} \\
    \gamma_{1i} = \beta_{10} +  \nu_{1i}
\end{aligned}
\end{equation}
```
Simulation A followed a 4 × 2 × 3 × 4 × 3 factorial design (yielding 288 simulation conditions in total), and Simulation B followed a 4 × 2 × 2 × 4 × 2 × 2 design (256 conditions in total). 1,000 replicates per cell (i.e., a combination of simulation conditions) were simulated. As such, 544,000 datasets were generated (and the same number of models was estimated) in this simulation study. The manipulated variables are listed in Table \ref{tab:tab_manipulated}, and the parameters that remained fixed throughout all simulation conditions are reported in Table \ref{tab:tab_stable}.

```{r tab_manipulated, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(kableExtra)
library(ggpubr)

r1 = c(
"Missingness pattern", "MCAR, block, extreme-onesided, extreme-twosided", "MCAR, block, extreme-onesided, extreme-twosided", "Simulated fixed AR effect", "0.3, 0.5, 0.7", "0.3, 0.7", "Variance of random AR effects", "-", "0.05, 0.1", "Compliance", "0.4, 0.6, 0.8, 1", "0.4, 0.6, 0.8, 1", "Number of participants (N)", "20, 50", "20, 50","Time points per participant (T.obs)", "20, 50, 100", "50, 100")
  
df_manipulated = matrix(r1, ncol = 3, byrow = TRUE)

kable(df_manipulated, booktabs = TRUE, align = "ccc", caption = "Values of the manipulated parameters used in the two simulation studies") %>% column_spec(c(1,2,3), width = "15em") %>% add_header_above(c("Manipulated parameter" = 1, "Simulation A" = 1, "Simulation B" = 1))
```

```{r tab_stable, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
r2 = c("Fixed intercept", "0", "0","Variance of random intercepts", "3", "3", "Innovation variance", "3", "3","Correlation between random intercepts and random slopes", "0", "0","Significance threshold", "0.05", "0.05","Simulation replicates per cell", "1000", "1000")

df_stable = matrix(r2, ncol = 3, byrow = TRUE)

kable(df_stable, booktabs = TRUE, align = "ccc", caption = "Parameters used for the two simulation studies.") %>% column_spec(c(1,2,3), width = "15em") %>% add_header_above(c("Simulation parameter" = 1, "Simulation A" = 1, "Simulation B" = 1))
```

**Data generation.** First, for each of the simulation conditions (i.e., combination of the parameters listed below), 1,000 synthetic datasets were generated. Each dataset contained observations from *N* simulated participants. A temporally dependent time-series of length *T.obs* was generated as nested within each simulated participant via a recursive equation. Additionally, for each time-series, a burn-in period containing 1,000 observation was generated and later discarded. The within-person error (innovation) vector $\epsilon_i$ was generated from a N(0, $\sigma$) distribution with $\sigma$ set to 3 in all simulations. The fixed intercept $\beta_{00}$ was set to 0 across all conditions. The random intercepts $\nu_{0i}$ for each simulated time-series were sampled from a *N*(0, 3) distribution in both studies. In Simulation A, only fixed autoregressive effects $\beta_{10}$ were simulated and manipulated, while both fixed autoregressive effects $\beta_{10}$ and random autoregressive effects $\nu_{1i}$ were included in Simulation B. No night gaps were assumed in the simulations. For an overview of the values of all manipulated simulation parameters, please refer to Table \ref{tab:tab_manipulated}.

Each time-series was then generated using Equation \eqref{eq:level1}. The initial value was generated as a sum of the person-specific intercept $\gamma_{0i}$ and the innovation $\epsilon_{ij}$, and the following observations were calculated by multiplying the value of the time-series at *t-1* by the person-specific autoregressive effect $\gamma_{1i}$ and adding the person-specific intercept $\gamma_{0i}$ and the innovation $\epsilon_{ij}$. Subsequently, after removing the burn-in datapoints, the first-order lagged version of the time-series was generated, setting the first lagged value as missing.

The non-manipulated simulation parameters ($\beta_{00}, \sigma_{\nu 0}, \sigma, \rho_\nu$) were set following a simulation design from Hamaker & Grasman [-@hamaker2015].

**Introduction of missing values.** After generating the time-series, missing data were introduced to each of the generated datasets according to the missing data pattern and compliance of the given simulation condition. Four different missingness patterns (corresponding to the hypothetical ESM study scenarios described in the Introduction) were introduced to the data:

*a) Data missing completely at random (MCAR)*. In the simulation conditions with data MCAR, (1-compliance) observations were set at missing using the *delete_MCAR* function from the *missMethods R* package [@rockel2022]. Each observation had an identical probability of being set as missing. For example, in the conditions with compliance of 0.6, each observation had probability of 0.4 of being set as missing. \
*b) Data missing in blocks of consecutive observations.* This missingness patterns corresponds to a simplified situation in which the participant misses multiple consecutive observations, for example because they cannot respond to ESM beeps during work or social occassions. One block of missing observations was introduced in each individual time-series. For each simulated participant, the block started with the 5th observation of their time-series, and its length was directly proportional to compliance. For example, in a condition with compliance of 0.6 an 100 timepoints per participant, 40 (100 × 0.4) observations were set as missing, from beep number 5 to beep number 45. \
*c) Most extreme observations, one side.* In missing patterns c) and d), missingness was directly dependent on the process value. In missing pattern c), all observations below the (100%-compliance) percentile were set as missing. For example, with compliance set to 0.6, all observations below the 40th percentile were set as missing. The value of the threshold quantile was computed individually for each time-series. \
*d)* *Most extreme observations set as missing, two sides.* In condition d), the highest and lowest (100%-compliance)/2 observations were set as missing. For example, with compliance set to 0.6, all observations below the 20th and above the 80th percentile were set as missing.

For an illustration of the different missing data patterns, see Figure 1.

It can be expected that the different missingness patterns will differ in their effects on the simulation outcomes (estimation bias, standard error, power). Even with an identical proportion of missing data, datasets with different missingness patterns will have different proportions of effective observation-pairs (i.e., proportion of time points for which both the observation at t and the observation at t-1 are not missing) used to estimate the autoregressive effect.

```{r fig_miss_comparison, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap = "\\label{fig:fig_miss_comparison}Illustration of the four different missingness pattern used in the simulation study. The blue dots represent observed datapoints, while the light gray dots represent missing values. Compliance is 0.7 in all four patterns. The total number of observations in each time-series is 100. There are 30 missing values in each time-series. The dashed red lines in plots c) and d) represent the threshold percentiles.", fig.height=6 }
library(dplyr)
library(missMethods)
library(ggplot2)

# Create the time-series 
arsim = function(N = 100, beta_0 = 0, beta_1 = 0.5, sigma = 0.5){
  set.seed(1)
  N_burning = 1000
  N_tot = N + N_burning
  error = rnorm(N_tot, 0, sigma)   
  mu = beta_0 / (1-beta_1)
  
  Y = rep(0, N_tot)  #[0 for i in 1:N_tot]
  
  Y[1]= mu + error[1]  # Initial value
  
  for (n in 2:N_tot){
    Y[n] = beta_0 + beta_1 * Y[n-1] + error[n]   # Simulate Dependent Variables
  }
  Y[(N_burning + 1):N_tot]
}

series = as.data.frame(arsim())

# 1) MCAR, compliance = 0.7 

# introduce MCAR to data 
tm = c(1:100)
series_mcar = delete_MCAR(ds = series, cols_mis = 1, p = 0.3)

data_mcar = data.frame(tm, series, series_mcar)
colnames(data_mcar) = c("time", "full_series", "series_mcar")
plot_MCAR = ggplot(data_mcar, aes(x = tm)) + 
                        
                        geom_line(aes(y = series_mcar),
                                  color = "blue",
                                  size = 1,
                                  alpha = 0.7) +
                        geom_point(aes(y = series_mcar),
                                   color = "blue",
                                   size = 1) + 
                        
                        geom_line(aes(y = full_series,
                                      alpha = 0.3),
                                  show.legend = FALSE) +
                        geom_point(aes(y = full_series,
                                       alpha = 0.25),
                                   show.legend = FALSE) +
                        ggtitle("a) Data missing completely at random") +
                        xlab("") +
                        ylab("Process value") + 
                        theme_minimal()


# 2) missing in blocks, compliance = 0.7
block_full = data_mcar$full_series
block_missing = data_mcar$full_series
df_block = data.frame(tm, block_full, block_missing)
df_block$block_missing[5:(5+(29))] = NA

plot_block = ggplot(df_block, aes(x = tm)) + 
    
    geom_line(aes(y = block_missing),
              color = "blue",
              size = 1,
              alpha = 0.7) +
    geom_point(aes(y = block_missing),
               color = "blue",
               size = 1) + 
    
    geom_line(aes(y = block_full,
                  alpha = 0.3),
              show.legend = FALSE) +
    geom_point(aes(y = block_full,
                   alpha = 0.25),
               show.legend = FALSE) +
    ggtitle("b) Block of consecutive missing datapoints") +
    xlab("") +
    ylab("Process value") + 
    theme_minimal()



# 3) extreme - onesided, compliance = 0.7 

threshold_quantile = quantile(data_mcar$full_series, 0.3)

oneside_full = data_mcar$full_series
oneside_missing = data_mcar$full_series
df_oneside = data.frame(tm, oneside_full, oneside_missing)
df_oneside$oneside_missing[df_oneside$oneside_missing < threshold_quantile] = NA

plot_oneside = ggplot(df_oneside, aes(x = tm)) + 
    
    geom_line(aes(y = oneside_missing),
              color = "blue",
              size = 1,
              alpha = 0.7) +
    geom_point(aes(y = oneside_missing),
               color = "blue",
               size = 1) + 
    
    geom_line(aes(y = oneside_full,
                  alpha = 0.3),
              show.legend = FALSE) +
    geom_point(aes(y = oneside_full,
                   alpha = 0.25),
               show.legend = FALSE) +
    geom_hline(yintercept = threshold_quantile, 
               linetype = "dashed", 
               color = "red") + 
    ggtitle("c) (1 - compliance) lowest values missing") +
    xlab("") +
    ylab("Process value") + 
    theme_minimal()




# 4) extreme - twosided, compliance = 0.7 
threshold_quantile_upper = quantile(data_mcar$full_series, 0.85)
threshold_quantile_lower = quantile(data_mcar$full_series, 0.15)


twoside_full = data_mcar$full_series
twoside_missing = data_mcar$full_series
df_twoside = data.frame(tm, twoside_full, twoside_missing)
df_twoside$twoside_missing[df_twoside$twoside_missing < threshold_quantile_lower] = NA
df_twoside$twoside_missing[df_twoside$twoside_missing > threshold_quantile_upper] = NA


plot_twoside = ggplot(df_twoside, aes(x = tm)) + 
    
    geom_line(aes(y = twoside_missing),
              color = "blue",
              size = 1,
              alpha = 0.7) +
    geom_point(aes(y = twoside_missing),
               color = "blue",
               size = 1) + 
    
    geom_line(aes(y = twoside_full,
                  alpha = 0.3),
              show.legend = FALSE) +
    geom_point(aes(y = twoside_full,
                   alpha = 0.25),
               show.legend = FALSE) +
    geom_hline(yintercept = threshold_quantile_lower, 
               linetype = "dashed", 
               color = "red") + 
    geom_hline(yintercept = threshold_quantile_upper, 
               linetype = "dashed", 
               color = "red") + 
    ggtitle("d) (1 - compliance)/2 lowest and highest values missing") +
    xlab("Beep") +
    ylab("Process value") + 
    theme_minimal()



(ggarrange(plot_MCAR, plot_block, plot_oneside, plot_twoside,
                 nrow = 4))
```

**Fitting a multilevel autoregressive model.** After missing values were introduced to the data, a MLAR(1) model was fitted to each of the simulated datasets using the *lme* function from the *nlme* R package [@pinheiro2022] with the value of the time-series at *t* as the outcome, the lagged *(t-1)* value of the time-series as the predictor, and the participant number as the grouping variable. We then extracted relevant parameters from the models that converged successfully. Missing values were treated by list-wise deletion. The restricted maximum log-likelihood method with the Broyden-Fletcher-Goldfarb-Shanno optimization algorithm was used to estimate the model.

Following the recommendations by Hamaker & Grasman [-@hamaker2015], the predictor (lagged variable) in the simulation was person-mean centered (i.e., the observed mean of each respective participant's ESM process was subtracted from the value of the lagged variable at each time point) in the main simulation studies. Although person-mean centering results in an underestimation of the autoregressive effect [@hamaker2015], it allows for a clearer interpretation of the within-person effects in multilevel models [@enders2007; @hamaker2020]. As a supplementary analysis, we also conducted the simulations without person-mean centering the predictor.

**Simulation outcomes.** Estimation bias, the standard error of the estimation, and the statistical power to estimate the fixed autoregressive effect $\beta_{10}$ were the focal outcomes of the study. Additionally, we examined the effect of the manipulated variables on the proportion of models that successfully converged and the bias in the estimation of the person-mean used for centering of the predictor (lagged) variable.

Estimation bias was computed as the difference between the estimated fixed autoregressive effect $\hat{\beta_{10}}$ and the real (simulated) fixed autoregressive effect $\beta_{10}$ in each simulation replicate. As such, the dataset with estimation bias contained 1,000 rows per simulation condition (the reported values are the average of the 1,000 results per conditions).

Standard error (SE) of the fixed autoregressive effect and statistical power were calculated for each simulation condition (i.e., 1 row per condition). Statistical power was computed as the proportion of simulation replicates (within the given simulation condition) in which the p-value for the estimated fixed autoregressive effect $\hat{\beta_{10}}$ was below the significance threshold ($\alpha$ = 0.05) and the number of simulation replicates that converged successfully.

The bias in the estimation of the person-mean of the time-series was computed as the average difference between the real process mean $\mu_i$ \eqref{eq:meanbias} and the observed person-mean $\hat{\mu_i}$ (computed after the missing data were introduced).

```{=tex}
\begin{equation}\label{eq:meanbias}
\begin{aligned}
    \mu_i = \frac{\beta_{00}+\nu_{0i}}{1-(\beta_{10}+\nu_{1i})}
\end{aligned}
\end{equation}
```
### Reproducibility and code/data availability

The simulations were conducted in R version 4.2.1 [@rcoreteam2021]. The study was conducted with emphasis on reproducibility of the results [@pawel2022]. As such, we provide all data (simulation results) used for the reported analyses, as well as the full reproducible R code for the simulations (including the custom functions created for the purposes of the study), and the code used to generate the plots and result tables (available at [https://github.com/benjsimsa/AR-missing-simulations)](https://github.com/benjsimsa/AR-missing-simulations) The repository also includes a *sessionInfo* document that lists the versions of the packages used for the study. The thesis was written using dynamic reporting in R Markdown [@allaire2022].

Additionally, the *renv* R package [@ushey2022] was used to set up a reproducible R environment and improve reproducibility by creating a project-local package library. For reproducible file referencing, the R package *here* [@müller2020] was used. For more information about the custom functions, simulation code, and the structure of the GitHub repository itself, please refer to the file README.md in the repository.
