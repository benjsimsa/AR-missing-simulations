---
title: "ANOVA question"
author: "b s"
date: "`r Sys.Date()`"
output: html_document
---

I have my results from study A (in which there were no random slopes): 4 missingness patterns \* 2 values of N \* 3 values of T \* 4 values of compliance \* 3 real (simulated) fixed AR effect sizes, 288 conditions in total.

Because we did not estimate random slopes, all models converged, so we have 1000 results (i.e., simulated datasets + fitted models) per condition. The dataframe **anova_df** contains 288,000 rows.

```{r}
anova_df = readRDS(here::here("anova_bias_df.rds"))
head(anova_df)
```

I would like to test how the factors we varied (missingness pattern, N, T, compliance, real AR) affect the estimation precision(variable *B1_bias,* i.e., the difference between the estimated and real fixed AR effect in each of the simulation runs), and whether there are interactions between these factors.

The question is: **what is the best way to do that?**

An obvious solution would be conducting an ANOVA. Of course, the problem is that given the huge sample size, even the smallest, unimportant difference will be significant (and we are not testing apriori hypotheses, so using p-values to determine which effects matter might be misguided anyway).

```{r}
library(effectsize)

anova_bias = aov(formula = B1_bias ~ N * T.obs * miss_type * compliance * B1_sim, 
    data = anova_df,
    type = 1)

(anova_bias_results = summary(anova_bias))
```

How do we then decide which main effects and interactions we care about?

Would it be reasonable to use some sort of cut-off for partial omega-squared (as a measure of effect size)? If so, what would be a reasonable cutoff?
```{r}
library(effectsize)

omega_squared(anova_bias)
```


Similar question applies to checking contrasts: given the big sample size and large number of comparisons that will be made, I don't think it makes sense to use p-values.

Also, we are interested in the MSE and power. These metrics are computed from the overall data from each run (i.e., we compute the average power for each run as the proportion of (times the model was significant)/(times the model converged). As such, the sample size is 288 here. Does it make sense to use ANOVA here as well, or do we stay with just descriptive tables / plots? 